<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shihao Zhou</title>

  <meta name="author" content="Shihao Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/circle-nodes-solid.svg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shihao Zhou (周世豪)</name>
              </p>
              <p>I am a Third-year PhD student at Nankai University where I am advised by Prof. <a href="https://cv.nankai.edu.cn/">Jufeng Yang</a>. Before that, I completed my Master at Xiamen University and obtained my BS degree at Zhejiang Gongshang University.
              </p>  
              <p>
                I am very fortunate to be co-advised by Prof. <a href="https://jspan.github.io/">Jinshan Pan</a> of Nanjing University of Science and Technology.
              </p>
              <p>
                I am interested in 3D Computer Vision and Low-level Vision problems.
              </p>

              <p style="text-align:center">
                <a href="mailto:shzhou@stu.xmu.edu.com">Email</a> &nbsp/&nbsp
                CV &nbsp/&nbsp
                Bio &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=MjOwkW8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/joshyZhou">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/self_shihaozhou.pdf"><img style="width:80%;max-width:80%" alt="profile photo" src="images/rounded_self_shihaozhou.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <hr>
              <p><strong><span class="label label-default">[Feb , 2025]</span> </strong>  <a href="https://github.com/Calvin11311/MDT">MDT</a> is accepted by CVPR 2025. Congrats to <a href="https://github.com/Calvin11311">Duosheng Chen</a></p>
              <p><strong><span class="label label-default">[Jul , 2024]</span> </strong>  <a href="https://github.com/joshyZhou/FPro">FPro</a> is accepted by ECCV 2024.</p>
              <p><strong><span class="label label-default">[Feb , 2024]</span> </strong>  <a href="https://github.com/joshyZhou/AST">AST</a> is accepted by CVPR 2024.</p>

              <!-- <tr>
                <th scope="row">Jul 2, 2024</th>
                <td>
                  
                  <a href="https://github.com/joshyZhou/FPro">FPro</a> is accepted by ECCV 2024.
      
                  
                </td>
              </tr>
              <tr>
                <td>
                    [Feb 27, 2024]<a href="https://github.com/joshyZhou/AST">AST</a> is accepted by CVPR 2024.
                </td>
              </tr> -->
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications (Selected)</heading>
            <hr>
          </td>
        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="MDT_stop()" onmouseover="MDT_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <div class="two" id='MDT_image'>
                  <img src='images/cvpr_MDT_on.png' width="160"></div>
                <img src='images/cvpr_MDT_off.png' width="160">
              </div>
              <script type="text/javascript">
                function MDT_start() {
                  document.getElementById('MDT_image').style.opacity = "1";
                }

                function MDT_stop() {
                  document.getElementById('MDT_image').style.opacity = "0";
                }
                AST_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <!-- <a href="data/PR_JSL3d_paper.pdf"> -->
                <papertitle>A Polarization-aided Transformer for Image Deblurring via Motion Vector Decomposition</papertitle>
              <!-- </a> -->
              <br>
              <!-- <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>, -->
							<a href="https://github.com/Calvin11311">Duosheng Chen</a>,
              <strong>Shihao Zhou</strong>,
              <a href="https://jspan.github.io/">Jinshan Pan</a>,
              <a href="https://jingleishi.github.io/">Jinglei Shi</a>,
              <a href="https://github.com/qulishen/">Lishen Qu</a>,
              <a href="https://cv.nankai.edu.cn/">Jufeng Yang</a>,
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em>
              <br>
              <a href="https://github.com/Calvin11311/MDT">project page </a>
              /
              <a href="https://arxiv.org/pdf/2404.00358">arXiv</a>
              /
              bibtex
              <p>
                we propose Motion Decomposition Transformer (MDT), a transformer-based architecture augmented with polarized modules for deblurring via motion vector decomposition.
              </p>
            </td>
          </tr>
          <tr onmouseout="FPro_stop()" onmouseover="FPro_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <div class="two" id='FPro_image'>
                  <img src='images/eccv_FPro_off.png' width="160"></div>
                <img src='images/eccv_FPro_on.png' width="160">
              </div>
              <script type="text/javascript">
                function FPro_start() {
                  document.getElementById('FPro_image').style.opacity = "1";
                }

                function FPro_stop() {
                  document.getElementById('FPro_image').style.opacity = "0";
                }
                AST_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <!-- <a href="data/PR_JSL3d_paper.pdf"> -->
                <papertitle>Seeing the Unseen: A Frequency Prompt Guided Transformer for Image Restoration</papertitle>
              <!-- </a> -->
              <br>
              <!-- <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>, -->
							<strong>Shihao Zhou</strong>,
              <a href="https://jspan.github.io/">Jinshan Pan</a>,
              <a href="https://jingleishi.github.io/">Jinglei Shi</a>,
              <a href="https://github.com/Calvin11311">Duosheng Chen</a>,
              <a href="https://github.com/qulishen/">Lishen Qu</a>,
              <a href="https://cv.nankai.edu.cn/">Jufeng Yang</a>,
              <!-- <a href="https://cs.xmu.edu.cn/info/1008/1152.htm">Yunqi Lei</a> -->
              <br>
              <em>European Conference on Computer Vision (ECCV), 2024</em>
              <br>
              <a href="https://github.com/joshyZhou/FPro">project page </a>
              /
              <a href="https://arxiv.org/pdf/2404.00288">arXiv</a>
              /
              <a href="data/ECCV24_FPro.bib">bibtex</a>
              <p>
                We develop a Frequency Prompting image restoration method, dubbed FPro, which can effectively provide prompt components from a frequency perspective to guild the restoration model.
              </p>
            </td>
          </tr>

          <tr onmouseout="AST_stop()" onmouseover="AST_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <div class="two" id='AST_image'>
                  <img src='images/cvpr_AST_off.png' width="160"></div>
                <img src='images/cvpr_AST_on.png' width="160">
              </div>
              <script type="text/javascript">
                function AST_start() {
                  document.getElementById('AST_image').style.opacity = "1";
                }

                function AST_stop() {
                  document.getElementById('AST_image').style.opacity = "0";
                }
                AST_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <!-- <a href="data/PR_JSL3d_paper.pdf"> -->
                <papertitle>Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration</papertitle>
              <!-- </a> -->
              <br>
              <!-- <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>, -->
							<strong>Shihao Zhou</strong>,
              <a href="https://github.com/Calvin11311">Duosheng Chen</a>,
              <a href="https://jspan.github.io/">Jinshan Pan</a>,
              <a href="https://jingleishi.github.io/">Jinglei Shi</a>,
              <a href="https://cv.nankai.edu.cn/">Jufeng Yang</a>,
              <!-- <a href="https://cs.xmu.edu.cn/info/1008/1152.htm">Yunqi Lei</a> -->
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
              <br>
              <!-- <a href="CVPR_AST_2024/index.html">project page</a> -->
              <a href="https://github.com/joshyZhou/AST">project page </a>
              /
              <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_Adapt_or_Perish_Adaptive_Sparse_Transformer_with_Attentive_Feature_Refinement_CVPR_2024_paper.html">pdf</a>
              /
              <a href="data/CVPR24_AST.bib">bibtex</a>
              <p>
              We present AST, an efficient Transformer-based model, that facilitates the flow of the most useful information forward, extracting more constructive features for the recovery of clear images.
              </p>
            </td>
          </tr>

          <tr onmouseout="JSL3d_stop()" onmouseover="JSL3d_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <div class="two" id='JSL3d_image'>
                  <img src='images/PR_JSL3d_on.jpeg' width="160"></div>
                <img src='images/PR_JSL3d_off.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function JSL3d_start() {
                  document.getElementById('JSL3d_image').style.opacity = "1";
                }

                function JSL3d_stop() {
                  document.getElementById('JSL3d_image').style.opacity = "0";
                }
                JSL3d_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="data/PR_JSL3d_paper.pdf">
                <papertitle>JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation</papertitle>
              </a>
              <br>
              <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>,
							<strong>Shihao Zhou</strong>,
              Cuihua Li,
              <a href="https://cs.xmu.edu.cn/info/1008/1152.htm">Yunqi Lei</a>
              <br>
              <em>Pattern Recognition (PR), 132(2022) 108965</em>
              <br>
              project page
              /
              <a href="data/PR_JSL3d_paper.pdf">pdf</a>
              /
              <a href="data/PR_JSL3d.bib">bibtex</a>
              <p>
              A  novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model.
              </p>
            </td>
          </tr>

            <tr onmouseout="DCGNet_stop()" onmouseover="DCGNet_start()">
              <td style="padding:20px;width:25%;vertical-align:top">
                <div class="one">
                  <div class="two" id='DCGNet_image'>
                    <img src='images/MM_DCGNet_on.png' width="160"></div>
                  <img src='images/MM_DCGNet_off.png' width="160">
                </div>
                <script type="text/javascript">
                  function DCGNet_start() {
                    document.getElementById('DCGNet_image').style.opacity = "1";
                  }

                  function DCGNet_stop() {
                    document.getElementById('DCGNet_image').style.opacity = "0";
                  }
                  DCGNet_stop()
                </script>
              </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/pdf/2108.12384.pdf">
                <papertitle>DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D Human Shape Reconstruction</papertitle>
              </a>
              <br>
              <strong>Shihao Zhou</strong>,
              <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>,
							Shanshan Cai,
              <a href="https://cs.xmu.edu.cn/info/1008/1152.htm">Yunqi Lei</a>
              <br>
              <em>ACM International Conference on Multimedia (MM), 2021</em> &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="MM_DCGNet_2021/index.html">project page</a>
              /
              <a href="data/MM_DCGnet_poster.pdf">poster</a>
              /
              <a href="data/MM_DCGNet_supply.pdf">supplementary</a>
              /
              <a href="https://arxiv.org/abs/2108.12384">arXiv</a>
              /
              <a href="data/MM_DCGNet.bib">bibtex</a>
              <p>
              Exploring deep relations within mesh vertices and proposing a shape completion task for human mesh reconstruction.
              </p>
            </td>
          </tr>

          <tr onmouseout="ACCV20_stop()" onmouseover="ACCV20_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <div class="one">
                <div class="two" id='ACCV20_image'>
                  <img src='images/ACCV_LocalitySimilarity_on.png' width="160"></div>
                <img src='images/ACCV_LocalitySimilarity.png' width="160">
              </div>
              <script type="text/javascript">
                function ACCV20_start() {
                  document.getElementById('ACCV20_image').style.opacity = "1";
                }

                function ACCV20_stop() {
                  document.getElementById('ACCV20_image').style.opacity = "0";
                }
                ACCV20_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
                <a href="data/ACCV_LocalitySimilarity_pdf.pdf">
                <papertitle>Towards Locality Similarity Preserving to 3D Human Pose Estimation</papertitle>
                </a>
              <br>
              <strong>Shihao Zhou</strong>,
              <a href="https://mengxijiang.github.io/">Mengxi Jiang</a>,
              Qicong Wang,
              <a href="https://cs.xmu.edu.cn/info/1008/1152.htm">Yunqi Lei</a>
              <br>
              <em>Asian Conference on Computer Vision (ACCV) Workshop, 2020</em>
              <br>
              <a href="ACCV_LSP_2020/index.html">project page</a>
              /
              <a href="data/ACCV_LocalitySimilarity_pdf.pdf">pdf</a>
              /
              <a href="data/10.1007_978-3-030-69756-3_10-citation.bib">bibtex</a>
              <p></p>
              <p>
              A simple example-based approach with locality similarity preserving to estimate 3D human pose.
              </p>

            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Fundings</heading>
              <hr>
         <p> 2024 National Natural Science Foundation (NSFC) Youth Basic Research Project (PhD). </p>
            </td>
          </tr>
        </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody><hr>
            <td style="padding:20px;width:25%;vertical-align:top">
            <div class="one">
              <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=Fi8vR1fw34Rxh2eGcSq70hpRu1rHJpIJJaSgLpnUgkU"></script>
            </div>
            </td>
            <td style="padding:20px;width:75%;;vertical-align:middle">
                <em>‘Whether I shall turn out to be the hero of my own life,
                <br>
                <br>
                or whether that station will be held by anybody else,
                <br>
                <br>
                these pages must show ‘</em>
            </td>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">This guy has an awesome website.</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
